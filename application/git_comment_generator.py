#!/usr/bin/env python3
import argparse
import random
import sys
import subprocess

import torch
from colorama import init, Fore, Style
from transformers import TextStreamer

from eval_model import setup_seed, init_model

init(autoreset=True)  # åˆå§‹åŒ– colorama ä»¥æ”¯æŒ Windows å’Œ ANSI é¢œè‰²


def print_help(msg=None):
    """æ‰“å°å¸®åŠ©ä¿¡æ¯ï¼Œå¸¦é¢œè‰²"""
    if msg:
        print(f"{Fore.RED}{msg}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}Usage:{Style.RESET_ALL}")
    print(f"  python {sys.argv[0]} 'git diff --cached'")


def main():
    args = argparse.Namespace(
        lora_name='None',
        out_dir='out',
        temperature=0.85,
        top_p=0.85,
        device='cuda' if torch.cuda.is_available()  else 'mps' if torch.backends.mps.is_available() else 'cpu',
        hidden_size=512,
        num_hidden_layers=8,
        max_seq_len=8192,
        use_moe=False,
        history_cnt=0,
        load=1,
        model_mode=2
    )
    # print('[DEBUG] args: ', args)
    # print(f"[DEBUG] sys.argv: {sys.argv}")
    print("[DEBUG] ç›®å‰ä½¿ç”¨çš„æ¨ç†è®¾å¤‡ä¸º", args.device)


    # è¯»å–å‘½ä»¤è¡Œå‚æ•°
    cmd_arg = None
    if len(sys.argv) > 1:
        cmd_arg = sys.argv[1]
        # print("[DEBUG] cmd_arg: ", cmd_arg)

    # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
    model, tokenizer = init_model(args)
    setup_seed(random.randint(0, 2048))

    # è·å¾—æµå¼è¾“å‡ºå™¨
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # è·å–è¾“å…¥å†…å®¹
    user_prompt = input("ğŸ‘¶: ")
    input_data = ""
    if cmd_arg:
        # æ‰§è¡Œå‘½ä»¤è·å– diff å†…å®¹
        result = subprocess.run(cmd_arg, shell=True, capture_output=True, text=True)
        input_data = result.stdout
        # print("[DEBUG] input_data: ", input_data)
    else:
        print_help("No git diff or text was passed to this function.")
        return 1

    # æ„é€  prompt
    # æç¤ºè¯å·¥ç¨‹
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªgitæäº¤ä¿¡æ¯ç”Ÿæˆæ¨¡å‹ï¼Œä½ éœ€è¦æ ¹æ®æˆ‘æä¾›çš„gitå·®å¼‚å†…å®¹ç”Ÿæˆ3æ¡æäº¤ä¿¡æ¯ã€‚åˆ†ç‚¹ï¼Œç”¨ä¸­æ–‡å›ç­”ã€‚ä¾‹å¦‚ï¼š
    1. æ·»åŠ äº†ä½•ç§åŠŸèƒ½
    2. ä¿®å¤äº†ä½•ç§bug
    3. ä¼˜åŒ–äº†ä½•ç§æ€§èƒ½
    
    è¯·æ ¹æ®ä»¥ä¸‹ç»™å®šçš„gitå·®å¼‚å†…å®¹ç”Ÿæˆ3æ¡æäº¤ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒç”¨æˆ·çš„æç¤º\"{user_prompt}\"ï¼š
    ```{input_data}```
    """
    # print('[DEBUG] prompt: ', prompt)

    # è°ƒç”¨minimindæœ¬åœ°æ¨¡å‹
    try:
        # æ„é€ messages
        messages = [{"role": "user", "content": prompt}]

        # å°†ç”¨æˆ·ä¸åŠ©æ‰‹ä¹‹é—´çš„å¤šè½®å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼(ä¸€ç§ç‰¹æ®Šçš„æ ¼å¼ï¼Œç±»ä¼¼äºjson)
        new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        ) if args.model_mode != 0 else (tokenizer.bos_token + prompt)

        # # å°†æ–‡æœ¬è¾“å…¥ new_prompt è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„å¼ é‡æ ¼å¼ï¼ˆtoken IDs å’Œ attention maskï¼‰
        # inputs = tokenizer(
        #     new_prompt,
        #     return_tensors="pt",
        #     truncation=True
        # ).to(args.device)

        try:
            # å°†æ–‡æœ¬è¾“å…¥ new_prompt è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„å¼ é‡æ ¼å¼ï¼ˆtoken IDs å’Œ attention maskï¼‰
            inputs = tokenizer(
                new_prompt,
                return_tensors="pt",
                truncation=True
            ).to(args.device)
        except Exception as e:
            if 'MPS' in str(e):
                print("æ£€æµ‹åˆ°MPSé”™è¯¯ï¼Œè‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼")
                # å°†æ–‡æœ¬è¾“å…¥ new_prompt è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„å¼ é‡æ ¼å¼ï¼ˆtoken IDs å’Œ attention maskï¼‰
                inputs = tokenizer(
                    new_prompt,
                    return_tensors="pt",
                    truncation=True
                ).to("cpu")
            else:
                raise e

        print('ğŸ¤–ï¸: ', end='')
        # å¾—åˆ°æ¨¡å‹è¾“å‡ºçš„tokenIds
        generated_ids = model.generate(
            inputs["input_ids"],
            max_new_tokens=args.max_seq_len,
            num_return_sequences=1,
            do_sample=True,
            attention_mask=inputs["attention_mask"],
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            streamer=streamer,
            top_p=args.top_p,
            temperature=args.temperature
        )
        # print('[DEBUG] generated_ids: ', generated_ids)

        # æ ¹æ®æ¨¡å‹ç”Ÿæˆçš„tokenIdsï¼Œç”Ÿæˆå¯¹åº”çš„response(æ–‡æœ¬)
        response = tokenizer.decode(generated_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        messages.append({"role": "assistant", "content": response})
        print('\n\n')
        # todo: é‡‡ç”¨git commentæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ˆæ•°æ®æ¥æºï¼šå®éªŒå®¤å†…éƒ¨å·²æœ‰çš„git commentï¼Œé€šè¿‡è„šæœ¬ç»„è£…æˆæ•°æ®é›†ï¼‰
        # todo: å¯ä»¥æ”¹è¿›çš„ç‚¹ï¼šå°†ä¸Šä¸€ç‰ˆå®Œæ•´ä»£ç è¿›è¡ŒRAGåå¬å›ç›¸å…³ä»£ç ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œæå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›
        return None
    except Exception as e:
        print(e)
        return None


if __name__ == "__main__":
    main()